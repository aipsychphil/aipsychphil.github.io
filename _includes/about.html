<section class="hero is-fullheight has-background-white-ter" id="about">
    <div class="hero-body">
        <div class="container">
            <div class="columns">
                <div class="column" markdown="1">
                    <h2>Description</h2>
                   <!--  <p>Be it in advice from a chatbot, suggestions on how to administer resources, or which content to
                      highlight, AI systems increasingly make value-laden decisions. However, researchers are becoming
                      increasingly concerned about whether AI systems are making the right decisions. These emerging
                      issues in the AI community have been long-standing topics of study in the fields of moral philosophy and moral psychology. Philosophers and psychologists have for decades (if not centuries) been interested in the systematic description and evaluation of human morality and the sub-problems that come up when attempting to describe and prescribe answers to moral questions. For instance,
                      philosophers and psychologists have long debated the merits of utility-based versus rule-based
                      theories of morality, their various merits and pitfalls, and the practical challenges of implementing them in resource-limited systems. They have pondered what to do in cases of moral uncertainty, attempted to enumerate all morally relevant concepts, and argued about what counts as a moral issue at all.</p>

                    <p>In some isolated cases, AI researchers have slowly started to adopt the theories, concepts, and tools
                      developed by moral philosophers and moral psychologists. For instance, we use the “trolley problem”
                      as a tool, adopt philosophical moral frameworks to tackle contemporary AI problems, and
                      have begun developing benchmarks that draw on psychological experiments probing moral judgment
                      and development.</p>

                    <p>Despite this, interdisciplinary dialogue remains limited. Each field uses specialized language, making
                      it difficult for AI researchers to adopt the theoretical and methodological frameworks developed
                      by philosophers and psychologists. Moreover, many theories in philosophy and psychology are
                      developed at a high level of abstraction and are not computationally precise. In order to overcome
                      these barriers, we need interdisciplinary dialog and collaboration. This workshop will create a venue
                      to facilitate these interactions by bringing together psychologists, philosophers, and AI researchers
                      working on morality. We hope that the workshop will be a jumping-off point for long-lasting
                      collaborations among the attendees and will break down barriers that currently divide the disciplines.</p> -->

                    <p>The central theme of the workshop will be the application of moral philosophy and moral psychology
                      theories to AI practices. Our invited speakers are some of the leaders in the emerging efforts to draw
                      on theories in philosophy or psychology to develop ethical AI systems. Their talks will demonstrate
                      cutting-edge efforts to do this cross-disciplinary work, while also highlighting their own shortcomings
                      (and those of the field more broadly). Each talk will receive a 5-minute commentary from a junior
                      scholar in a field that is different from that of the speaker. We hope these talks and commentaries will
                      inspire conversations among the rest of the attendees.</p>

                    <h2>Structure</h2>

                    <p>The core of the workshop will be a series of in-person invited talks from
                    leading scholars working at the intersection of AI, psychology, and philosophy on issues related to morality. Each talk will be followed by a 5-minute comment by a junior scholar whose training is primarily in a field that is different from the speaker’s field. This format will encourage interdisciplinary exchange. The day will end with a panel discussion of all the speakers. We will also organize two
                    poster sessions (of contributed papers) to ensure individual interaction between the attendees and
                    presenters.</p>

                    <h2>Confirmed speakers and tentative talk topics</h2>

                    <div class="row">
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns ">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/weidinger.jpg" alt="Laura Weidinger">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Laura Weidinger</strong> (Senior Research Scientist, DeepMind, AI + Psychology): The use of
                                findings in developmental moral psychology to create benchmarks for an AI system’s moral
                                competence.</p>
                            </div>
                        </div>
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/tenenbaum.jpeg" alt="Josh Tenenbaum">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Josh Tenenbaum</strong> (Professor, MIT, AI + Psychology): Using a recent “contractualist” theory
                                of moral cognition  to lay a roadmap for developing an AI system that makes human-like
                                moral judgments.</p>
                            </div>
                        </div>
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/bowman.jpg" alt="Sam Bowman">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Sam Bowman</strong> (Associate Professor, NYU & Anthropic, AI): Using insights from cognitive
                        science for language model alignment.</p>
                            </div>
                        </div>
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/sinnott.jpg" alt="Walter Sinnott-Armstrong">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Walter Sinnott-Armstrong</strong> (Professor, Duke, AI + Philosophy): Using preference-elicitation techniques to align kidney allocation algorithms with human values.</p>
                            </div>
                        </div>
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/rini.jpg" alt="Regina Rini">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Regina Rini</strong>(Associate Professor, York University, Philosophy): The use of John Rawls’ “decision procedure for ethics”  as a guiding framework for crowdsourcing ethical
                        judgments to be used as training data for large language models.</p>
                            </div>
                        </div>
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/greene.jpg" alt="Josh Greene">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Josh Greene</strong> (Professor, Harvard, Psychology): An approach to AI safety and ethics inspired
                        by the human brain’s dual-process (“System 1/System 2”) architecture.</p>
                            </div>
                        </div>
                        <div class="col-sm-4 col-xs-12">
                            <div class="column columns">
                                <figure class="image">
                                    <img class="is-rounded" src="{{site.baseurl}}/assets/images/saxe.jpg" alt="Rebecca Saxe">
                                </figure>
                            </div>
                            <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">
                                <p><strong>Rebecca Saxe</strong> (Professor, MIT, Psychology): Using the neuroscience of theory-of-mind to
                        build socially and ethically aware AI systems.</p>
                            </div>
                        </div>
                    </div>

                    <h4>Tentative schedule in CDT (subject to change):</h4>

                    <p>9:00 am - 9:10 am  Opening remarks</p>
                    <p>9:10 am - 9:45 am Invited talk #1 + QA</p>
                    <p>9:45 am - 10:45 am Poster Session #1 (Contributed papers)</p>
                    <p>10:45 - 10:50 am Coffee Break</p>
                    <p>10:50 am -11:25 am Invited talk #2 + QA</p>
                    <p>11:25 am - 12:00 pm Invited talk #3 + QA</p>
                    <p>12:00 pm - 1:30 pm Lunch</p>
                    <p>1:30 pm - 2:05 pm Invited talk #4 + QA</p>
                    <p>2:05 pm - 2:40 pm Invited talk #5 + QA</p>
                    <p>2:40 pm - 2:50 pm Coffee Break</p>
                    <p>2:50 pm - 3:50 pm Poster session #2 (Contributed papers)</p>
                    <p>3:50 pm - 4:25 pm Invited talk #6 + QA</p>
                    <p>4:25 pm - 5:00 pm Invited talk #7 + QA</p>
                    <p>5:00 - 6:00 pm Panel discussion with all speakers & closing remarks</p>


                </div>
            </div>
        </div>
</section>