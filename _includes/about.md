<section class="hero is-fullheight has-background-white-ter" id="about">
    <div class="hero-body">
        <div class="container">
            <div class="columns">
                <div class="column" markdown="1">

                <h2>Description</h2>
                <p>Be it in advice from a chatbot, suggestions on how to administer resources, or which content to
                  highlight, AI systems increasingly make value-laden decisions. However, researchers are becoming
                  increasingly concerned about whether AI systems are making the right decisions. These emerging
                  issues in the AI community have been long-standing topics of study in the fields of moral philosophy and moral psychology. Philosophers and psychologists have for decades (if not centuries) been interested in the systematic description and evaluation of human morality and the sub-problems that come up when attempting to describe and prescribe answers to moral questions. For instance,
                  philosophers and psychologists have long debated the merits of utility-based versus rule-based
                  theories of morality, their various merits and pitfalls, and the practical challenges of implementing them in resource-limited systems [4 ]. They have pondered what to do in cases of moral uncertainty [7 , 3], attempted to enumerate all morally relevant concepts [ 2], and argued about what counts as a moral issue at all [8].</p>

                <p>In some isolated cases, AI researchers have slowly started to adopt the theories, concepts, and tools
                  developed by moral philosophers and moral psychologists. For instance, we use the “trolley problem”
                  as a tool [ 1], adopt philosophical moral frameworks to tackle contemporary AI problems [ 5, 10], and
                  have begun developing benchmarks that draw on psychological experiments probing moral judgment
                  and development [11].</p>

                <p>Despite this, interdisciplinary dialogue remains limited. Each field uses specialized language, making
                  it difficult for AI researchers to adopt the theoretical and methodological frameworks developed
                  by philosophers and psychologists. Moreover, many theories in philosophy and psychology are
                  developed at a high level of abstraction and are not computationally precise. In order to overcome
                  these barriers, we need interdisciplinary dialog and collaboration. This workshop will create a venue
                  to facilitate these interactions by bringing together psychologists, philosophers, and AI researchers
                  working on morality. We hope that the workshop will be a jumping-off point for long-lasting
                  collaborations among the attendees and will break down barriers that currently divide the disciplines.</p>

                <p>The central theme of the workshop will be the application of moral philosophy and moral psychology
                  theories to AI practices. Our invited speakers are some of the leaders in the emerging efforts to draw
                  on theories in philosophy or psychology to develop ethical AI systems. Their talks will demonstrate
                  cutting-edge efforts to do this cross-disciplinary work, while also highlighting their own shortcomings
                  (and those of the field more broadly). Each talk will receive a 5-minute commentary from a junior
                  scholar in a field that is different from that of the speaker. We hope these talks and commentaries will
                  inspire conversations among the rest of the attendees.</p>

                <h2>Structure</h2>

                <p>The core of the workshop will be a series of in-person invited talks from
                leading scholars working at the intersection of AI, psychology, and philosophy on issues related to morality. Each talk will be followed by a 5-minute comment by a junior scholar whose training is primarily in a field that is different from the speaker’s field. This format will encourage interdisciplinary exchange. The day will end with a panel discussion of all the speakers. We will also organize two
                poster sessions (of contributed papers) to ensure individual interaction between the attendees and
                presenters.</p>

                <h2>Confirmed speakers and tentative talk topics</h2>

                <ul>
                    <li><strong>Laura Weidinger</strong> (Senior Research Scientist, DeepMind, AI + Psychology): The use of
                    findings in developmental moral psychology to create benchmarks for an AI system’s moral
                    competence.</li>
                    <li><strong>Josh Tenenbaum</strong> (Professor, MIT, AI + Psychology): Using a recent “contractualist” theory
                    of moral cognition [6] to lay a roadmap for developing an AI system that makes human-like
                    moral judgments.</li>
                    <li><strong>Sam Bowman</strong> (Associate Professor, NYU & Anthropic, AI): Using insights from cognitive
                    science for language model alignment.</li>
                    <li><strong>Walter Sinnott-Armstrong</strong> (Professor, Duke, AI + Philosophy): Using preference-elicitation techniques to align kidney allocation algorithms with human values.</li>
                    <li><strong>Regina Rini</strong>(Associate Professor, York University, Philosophy): The use of John Rawls’ “decision procedure for ethics” [9] as a guiding framework for crowdsourcing ethical
                    judgments to be used as training data for large language models.</li>
                    <li><strong>Josh Greene</strong> (Professor, Harvard, Psychology): An approach to AI safety and ethics inspired
                    by the human brain’s dual-process (“System 1/System 2”) architecture.</li>
                    <li><strong>Rebecca Saxe</strong> (Professor, MIT, Psychology): Using the neuroscience of theory-of-mind to
                    build socially and ethically aware AI systems.</li>
                </ul>

                </div>
            </div>
        </div>
</section>